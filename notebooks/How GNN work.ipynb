{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How GNN work\n",
    "\n",
    "In this tutorial, we will explore graph neural networks and graph convolutions. **Graphs are a super general representation of data with intrinsic structure**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decomposing features(signal) and structure\n",
    "\n",
    "The key concept to understand graphs is **the decomposition of structure and signal (features)**, which make them so powerful and general methods.\n",
    "\n",
    "`Graphs are not any different: they are data with decomposed structure and signal information.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-world signals that we can model with graphs\n",
    "\n",
    "As long as you can define these two representations, you can model anything you want with graphs.\n",
    "\n",
    "Formally, the words or pixels are simply nodes, denoted by $N$. The connectivity/structure will be defined by a $N \\times N$ matrix, the so-called **Adjacency matrix** $A$. The element $i, j$ of $A$ will tell us if node $i$ is connected to node $j$.\n",
    "\n",
    "The signal/features for eaach node will be $X \\in R^{N \\times F}$. $F$ is the number of features.\n",
    "\n",
    "![example](https://theaisummer.com/static/692e414df0c37c4becd39593f5ca8d2d/82b28/graph-structure-signal.png)\n",
    "\n",
    "**Tips**: Note that the diagonal of $A$ is shown to contain ones, which is usually the case in graph neural networks **for training stability reasons**, although in the general case it has zeros, indicating no-self connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The basic maths for processing graph-structured data\n",
    "\n",
    "A very important and practical feature is the **degree** of each node, which is simply **the number of nodes that it is connected to**.\n",
    "\n",
    "If $A$ is binary the degree corresponds to the number of neighbors in the graph.\n",
    "\n",
    "**Tips**: Since the degree corresponds to some kind of feature that is linked to the node, it is more convenient to place the degree vector in a diagonal $N \\times N$ matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 3., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand(3, 3)\n",
    "a[a > 0.5] = 1\n",
    "a[a <= 0.5] = 0\n",
    "\n",
    "def calc_degree_matrix(a):\n",
    "    return torch.diag(a.sum(dim=-1))\n",
    "\n",
    "d = calc_degree_matrix(a)\n",
    "\n",
    "print(a)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The degree matrix $D$ is used for the computation of the most important graph operator: **the graph Laplacian**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The grapg Laplacian\n",
    "\n",
    "The graph Laplacian is defined as:\n",
    "$$L = D - A$$\n",
    "\n",
    "In fact, the diagonal elements of $L$ will have the degree of the node, if $A$ has no self-loops.\n",
    "\n",
    "On the other hand, the non-diagonal elements $L_ij = -1$ , when $i \\ne j $ if there is a connection. If there is **no** connection $L_ij = 0$, when $i \\ne j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1., 1., 1.],\n",
      "        [1., 0., 1., 0., 1.],\n",
      "        [1., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  0., -1., -1., -1.],\n",
       "        [-1.,  3., -1.,  0., -1.],\n",
       "        [-1.,  0.,  2.,  0., -1.],\n",
       "        [ 0., -1.,  0.,  1.,  0.],\n",
       "        [ 0.,  0., -1., -1.,  2.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_graph_lap(a):\n",
    "    return calc_degree_matrix(a) - a\n",
    "\n",
    "L = calc_graph_lap(a)\n",
    "\n",
    "a = torch.Tensor(\n",
    "    [\n",
    "        [0., 0., 1., 1., 1.],\n",
    "        [1., 0., 1., 0., 1.],\n",
    "        [1., 0., 0., 0., 1.],\n",
    "        [0., 1., 0., 0., 0.],\n",
    "        [0., 0., 1., 1., 0.]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(a)\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in graph neural networks we use its **normalized version**. \n",
    "\n",
    "`Why?`\n",
    "\n",
    "Because nodes have varying connectivity and as a result a big range of degree values on $D$.\n",
    "\n",
    "This will create huge problems when processing with gradient-based methods.\n",
    "\n",
    "$$L_norm = D^{-\\frac {1}{2}} L D^{-\\frac {1}{2}} = I - D^{-\\frac {1}{2}} A D^{-\\frac {1}{2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1., 1., 1.],\n",
      "        [1., 0., 1., 0., 1.],\n",
      "        [1., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 0.5918, 0.4226, 0.5918],\n",
       "        [0.6667, 1.0000, 0.5918, 1.0000, 0.5918],\n",
       "        [0.5918, 1.0000, 1.0000, 1.0000, 0.5000],\n",
       "        [1.0000, 0.4226, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 0.5000, 0.2929, 1.0000]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_degree_matrix_norm(a):\n",
    " return torch.diag(torch.pow(a.sum(dim=-1),-0.5))\n",
    "\n",
    "def create_graph_lapl_norm(a):\n",
    "    size = a.shape[-1]\n",
    "    D_norm = calc_degree_matrix_norm(a)\n",
    "    L_norm = torch.ones(size) - (D_norm @ a @ D_norm )\n",
    "    \n",
    "    return L_norm\n",
    "\n",
    "L_norm = create_graph_lapl_norm(a)\n",
    "\n",
    "print(a)\n",
    "L_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the diagonal elements will be ones when there is at least one connection of the graph’s node independent of the node’s degree.\n",
    "\n",
    "The node’s degree will now influence the non-diagonal elements which will be: $\\frac {1}{(deg(n_i)deg(n_j))^{\\frac {1}{2}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In graph neural networks a slightly alternated version is often used:\n",
    "\n",
    "$$L_{norm}^{mod} = D^{-\\frac {1}{2}} (A + I) D^{-\\frac {1}{2}}$$\n",
    "\n",
    "where $I$ denotes the **identity matrix**, which adds self-connections.\n",
    "\n",
    "From now on, we will refer to this as a `normalized graph laplacian`.\n",
    "\n",
    "With this trick, **the input can be fed into a gradient-based algorithm without causing instabilities**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplacian eigenvalues and eigenvectors(特征值 & 特征向量)\n",
    "\n",
    "In the most common case, **a graph that has a single zero eigenvalue is connected**, meaning that starting from any node you can visit all the other nodes with some steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How graph convolutions layer are formed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simplest case we have the GCN layer:\n",
    "\n",
    "$$Y = L_norm X W$$\n",
    "\n",
    "$$L_{norm}^{mod} = D^{-\\frac {1}{2}} (A + I) D^{-\\frac {1}{2}}$$\n",
    "\n",
    "In this case, each layer will consider only its **direct neighbors** since we use the first power of laplacian $L^1$. This is similar to a $3 \\times 3$ kernel in classical image convolution, wherein we aggregate information from the direct pixel’s neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def device_as(x, y):\n",
    "    return x.to(y.device)\n",
    "\n",
    "# tensor operations now support batched inputs\n",
    "def calc_degree_matrix_norm(a):\n",
    "    return torch.diag_embed(torch.pow(a.sum(dim=-1),-0.5))\n",
    "\n",
    "def create_graph_lapl_norm(a):\n",
    "    size = a.shape[-1]\n",
    "    a +=  device_as(torch.eye(size),a)\n",
    "    D_norm = calc_degree_matrix_norm(a)\n",
    "    L_norm = torch.bmm( torch.bmm(D_norm, a) , D_norm )\n",
    "    return L_norm\n",
    "\n",
    "class GCN_AISUMMER(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "\n",
    "    def forward(self, X, A):\n",
    "        \"\"\"\n",
    "        A: adjecency matrix\n",
    "        X: graph signal\n",
    "        \"\"\"\n",
    "        L = create_graph_lapl_norm(A)\n",
    "        x = self.linear(X)\n",
    "        return torch.bmm(L, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the total graph neural network architecture that we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                        in_features = 7,\n",
    "                        hidden_dim = 64,\n",
    "                        classes = 2,\n",
    "                        dropout = 0.5):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.conv1 = GCN_AISUMMER(in_features, hidden_dim)\n",
    "        self.conv2 = GCN_AISUMMER(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCN_AISUMMER(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, classes)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x,A):\n",
    "        x = self.conv1(x, A)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, A)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, A)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        # aggregate node embeddings\n",
    "        x = x.mean(dim=1)\n",
    "        # final classification layer\n",
    "        return self.fc(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "672febc8a634011916ed982843df2ba95c600e4486f28df059bab71366202ec7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
